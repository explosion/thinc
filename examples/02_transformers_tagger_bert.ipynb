{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a part-of-speech tagger with transformers (BERT)\n",
    "\n",
    "This example shows how to use Thinc and Hugging Face's [`transformers`](https://github.com/huggingface/transformers) library to implement and train a part-of-speech tagger on the Universal Dependencies [AnCora corpus](https://github.com/UniversalDependencies/UD_Spanish-AnCora). This notebook assumes familiarity with machine learning concepts, transformer models and Thinc's config system and `Model` API (see the \"Thinc for beginners\" notebook and the [documentation](https://thinc.ai/docs) for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install thinc transformers torch ml_datasets tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use Thinc's `prefer_gpu` helper to make sure we're performing operations **on GPU if available**. The function should be called right after importing Thinc, and it returns a boolean indicating whether the GPU has been activated. If we're on GPU, we can also call `use_pytorch_for_gpu_memory` to route `cupy`'s memory allocation via PyTorch, so both can play together nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import prefer_gpu, use_pytorch_for_gpu_memory\n",
    "\n",
    "is_gpu = prefer_gpu()\n",
    "print(\"GPU:\", is_gpu)\n",
    "if is_gpu:\n",
    "    use_pytorch_for_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: the final config\n",
    "\n",
    "Here's the final config for the model we're building in this notebook. It references a custom `TransformersTagger` that takes the name of a starter (the pretrained model to use), an optimizer, a learning rate schedule with warm-up and the general training settings. You can keep the config string within your file or notebook, or save it to a `conig.cfg` file and load it in via `Config.from_disk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"\"\"\n",
    "[model]\n",
    "@layers = \"TransformersTagger.v0\"\n",
    "starter = \"bert-base-multilingual-cased\"\n",
    "\n",
    "[optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "\n",
    "[optimizer.learn_rate]\n",
    "@schedules = \"warmup_linear.v1\"\n",
    "initial_rate = 0.01\n",
    "warmup_steps = 3000\n",
    "total_steps = 6000\n",
    "\n",
    "[loss]\n",
    "@losses = \"SequenceCategoricalCrossentropy.v0\"\n",
    "\n",
    "[training]\n",
    "batch_size = 128\n",
    "words_per_subbatch = 2000\n",
    "n_epoch = 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the model\n",
    "\n",
    "The Thinc model we want to define should consist of 3 components: the transformers **tokenizer**, the actual **transformer** implemented in PyTorch and a **softmax-activated output layer**.\n",
    "\n",
    "\n",
    "### 1. Wrapping the tokenizer\n",
    "\n",
    "To make it easier to keep track of the data that's passed around (and get type errors if something goes wrong), we first create a `TokensPlus` dataclass that holds the output of the `batch_encode_plus` method of the `transformers` tokenizer. You don't _have to_ do this, but it makes things easier, can prevent bugs and helps the type checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class TokensPlus:\n",
    "    input_ids: torch.Tensor\n",
    "    token_type_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    input_len: List[int]\n",
    "    overflowing_tokens: Optional[torch.Tensor] = None\n",
    "    num_truncated_tokens: Optional[torch.Tensor] = None\n",
    "    special_tokens_mask: Optional[torch.Tensor] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapped tokenizer will take a list-of-lists as input (the texts) and will output a `TokensPlus` object containing the fully padded batch of tokens. The wrapped transformer will take a list of `TokensPlus` objects and will output a list of 2-dimensional arrays.\n",
    "\n",
    "1. **TransformersTokenizer**: `List[List[str]]` â†’ `TokensPlus`\n",
    "2. **Transformer**: `TokensPlus` â†’ `List[Array2d]`\n",
    "\n",
    "> ðŸ’¡ Since we're adding type hints everywhere (and Thinc is fully typed, too), you can run your code through [`mypy`](https://mypy.readthedocs.io/en/stable/) to find type errors and inconsistencies. If you're using an editor like Visual Studio Code, you can enable `mypy` linting and type errors will be highlighted in real time as you write code.\n",
    "\n",
    "To wrap the tokenizer, we register a new function that returns a Thinc `Model`. The function takes the name of the pretrained weights (e.g. `\"bert-base-multilingual-cased\"`) as an argument that can later be provided via the config. After loading the `AutoTokenizer`, we can stash it in the attributes. This lets us access it at any point later on via `model.get_attr(\"tokenizer\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc\n",
    "from thinc.api import Model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "@thinc.registry.layers(\"transformers_tokenizer.v0\")\n",
    "def TransformersTokenizer(name: str) -> Model[List[List[str]], TokensPlus]:\n",
    "    def forward(model, texts: List[List[str]], is_train: bool):\n",
    "        tokenizer = model.get_attr(\"tokenizer\")\n",
    "        token_data = tokenizer.batch_encode_plus(\n",
    "            [(text, None) for text in texts],\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_masks=True,\n",
    "            return_input_lengths=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return TokensPlus(**token_data), lambda d_tokens: []\n",
    "\n",
    "    return Model(\"tokenizer\", forward, attrs={\"tokenizer\": AutoTokenizer.from_pretrained(name)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass takes the model and a list-of-lists of strings and outputs the `TokensPlus` dataclass and a callback to use during the backwards (which does nothing in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wrapping the transformer\n",
    "\n",
    "To load and wrap the transformer, we can use `transformers.AutoModel` and Thinc's `PyTorchWrapper`. The forward method of the wrapped model can take arbitrary positional arguments and keyword arguments. Here's what the wrapped model is going to look like:\n",
    "\n",
    "```python\n",
    "@thinc.registry.layers(\"transformers_model.v0\")\n",
    "def Transformer(name) -> Model[TokensPlus, List[Array2d]]:\n",
    "    return PyTorchWrapper(\n",
    "        AutoModel.from_pretrained(name),\n",
    "        convert_inputs=convert_transformer_inputs,\n",
    "        convert_outputs=convert_transformer_outputs,\n",
    "    )\n",
    "```\n",
    "\n",
    "The transformer takes `TokensPlus` data as input (as produced by the tokenizer) and outputs a list of 2-dimensional arrays. The convert functions are used to **map inputs and outputs to and from the PyTorch model**. Each function should return the converted output, and a callback to use during the backward pass. To make the arbitrary positional and keyword arguments easier to manage, Thinc uses an `ArgsKwargs` dataclass, essentially a named tuple with `args` and `kwargs` that can be spread into a function as `*ArgsKwargs.args` and `**ArgsKwargs.kwargs`. The `ArgsKwargs` objects will be passed straight into the model in the forward pass, and straight into `torch.autograd.backward` during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import ArgsKwargs, torch2xp, xp2torch\n",
    "from thinc.types import Array2d\n",
    "\n",
    "def convert_transformer_inputs(model, tokens: TokensPlus, is_train):\n",
    "    kwargs = {\n",
    "        \"input_ids\": tokens.input_ids,\n",
    "        \"attention_mask\": tokens.attention_mask,\n",
    "        \"token_type_ids\": tokens.token_type_ids,\n",
    "    }\n",
    "    return ArgsKwargs(args=(), kwargs=kwargs), lambda dX: []\n",
    "\n",
    "\n",
    "def convert_transformer_outputs(model, inputs_outputs, is_train):\n",
    "    layer_inputs, torch_outputs = inputs_outputs\n",
    "    torch_tokvecs: torch.Tensor = torch_outputs[0]\n",
    "    torch_outputs = None  # free the memory as soon as we can\n",
    "    lengths = list(layer_inputs.input_len)\n",
    "    tokvecs: List[Array2d] = model.ops.unpad(torch2xp(torch_tokvecs), lengths)\n",
    "    tokvecs = [arr[1:-1] for arr in tokvecs]  # remove the BOS and EOS markers\n",
    "\n",
    "    def backprop(d_tokvecs: List[Array2d]) -> ArgsKwargs:\n",
    "        # Restore entries for BOS and EOS markers\n",
    "        row = model.ops.alloc_f2d(1, d_tokvecs[0].shape[1])\n",
    "        d_tokvecs = [model.ops.xp.vstack((row, arr, row)) for arr in d_tokvecs]\n",
    "        return ArgsKwargs(\n",
    "            args=(torch_tokvecs,),\n",
    "            kwargs={\"grad_tensors\": xp2torch(model.ops.pad(d_tokvecs))},\n",
    "        )\n",
    "\n",
    "    return tokvecs, backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returned by `AutoModel.from_pretrained` is a PyTorch model we can wrap with Thinc's `PyTorchWrapper`. The converter functions tell Thinc how to transform the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc\n",
    "from thinc.api import PyTorchWrapper\n",
    "from transformers import AutoModel\n",
    "\n",
    "@thinc.registry.layers(\"transformers_model.v0\")\n",
    "def Transformer(name: str) -> Model[TokensPlus, List[Array2d]]:\n",
    "    return PyTorchWrapper(\n",
    "        AutoModel.from_pretrained(name),\n",
    "        convert_inputs=convert_transformer_inputs,\n",
    "        convert_outputs=convert_transformer_outputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the `TransformersTokenizer` and `Transformer` into a feed-forward network using the `chain` combinator. The `with_array` layer transforms a sequence of data into a contiguous 2d array on the way into and\n",
    "out of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import chain, with_array, Softmax\n",
    "\n",
    "@thinc.registry.layers(\"TransformersTagger.v0\")\n",
    "def TransformersTagger(starter: str, n_tags: int = 17) -> Model[List[List[str]], List[Array2d]]:\n",
    "    return chain(\n",
    "        TransformersTokenizer(starter),\n",
    "        Transformer(starter),\n",
    "        with_array(Softmax(n_tags)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training the model\n",
    "\n",
    "### Setting up model and data\n",
    "\n",
    "Since we've registered all layers via `@thinc.registry.layers`, we can construct the model, its settings and other functions we need from a config (see `CONFIG` above). The result is a config object with a model, an optimizer, a function to calculate the loss and the training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import Config, registry\n",
    "\n",
    "C = registry.make_from_config(Config().from_str(CONFIG))\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = C[\"model\"]\n",
    "optimizer = C[\"optimizer\"]\n",
    "calculate_loss = C[\"loss\"]\n",
    "cfg = C[\"training\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ve prepared a separate package [`ml-datasets`](https://github.com/explosion/ml-datasets) with loaders for some common datasets, including the AnCora data. If we're using a GPU, calling `ops.asarray` on the outputs ensures that they're converted to `cupy` arrays (instead of `numpy` arrays). Calling `Model.initialize` with a batch of inputs and outputs allows Thinc to **infer the missing dimensions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_datasets\n",
    "(train_X, train_Y), (dev_X, dev_Y) = ml_datasets.ud_ancora_pos_tags()\n",
    "\n",
    "train_Y = list(map(model.ops.asarray, train_Y))  # convert to cupy if needed\n",
    "dev_Y = list(map(model.ops.asarray, dev_Y))  # convert to cupy if needed\n",
    "\n",
    "model.initialize(X=train_X[:5], Y=train_Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for training and evaluation\n",
    "\n",
    "Before we can train the model, we also need to set up the following helper functions for batching and evaluation:\n",
    "\n",
    "* **`minibatch_by_words`:** Group pairs of sequences into minibatches under `max_words` in size, considering padding. The size of a padded batch is the length of its longest sequence multiplied by the number of elements in the batch.\n",
    "* **`evaluate_sequences`:** Evaluate the model sequences of two-dimensional arrays and return the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_by_words(pairs, max_words):\n",
    "    pairs = list(zip(*pairs))\n",
    "    pairs.sort(key=lambda xy: len(xy[0]), reverse=True)\n",
    "    batch = []\n",
    "    for X, Y in pairs:\n",
    "        batch.append((X, Y))\n",
    "        n_words = max(len(xy[0]) for xy in batch) * len(batch)\n",
    "        if n_words >= max_words:\n",
    "            yield batch[:-1]\n",
    "            batch = [(X, Y)]\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def evaluate_sequences(model, Xs: List[Array2d], Ys: List[Array2d], batch_size: int) -> float:\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for X, Y in model.ops.multibatch(batch_size, Xs, Ys):\n",
    "        Yh = model.predict(X)\n",
    "        for yh, y in zip(Yh, Y):\n",
    "            correct += (y.argmax(axis=1) == yh.argmax(axis=1)).sum()\n",
    "            total += y.shape[0]\n",
    "    return float(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop\n",
    "\n",
    "Transformers often learn best with **large batch sizes** â€“ larger than fits in GPU memory. But you don't have to backprop the whole batch at once. Here we consider the \"logical\" batch size (number of examples per update) separately from the physical batch size. For the physical batch size, what we care about is the **number of words** (considering padding too). We also want to sort by length, for efficiency. \n",
    "\n",
    "At the end of the batch, we **call the optimizer** with the accumulated gradients, and **advance the learning rate schedules**. You might want to evaluate more often than once per epoch â€“ that's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from thinc.api import fix_random_seed\n",
    "\n",
    "fix_random_seed(0)\n",
    "\n",
    "for epoch in range(cfg[\"n_epoch\"]):\n",
    "    batches = model.ops.multibatch(cfg[\"batch_size\"], train_X, train_Y, shuffle=True)\n",
    "    for outer_batch in tqdm(batches, leave=False):\n",
    "        for batch in minibatch_by_words(outer_batch, cfg[\"words_per_subbatch\"]):\n",
    "            inputs, truths = zip(*batch)\n",
    "            guesses, backprop = model(inputs, is_train=True)\n",
    "            backprop(calculate_loss.get_grad(guesses, truths))\n",
    "        model.finish_update(optimizer)\n",
    "        optimizer.step_schedules()\n",
    "    score = evaluate_sequences(model, dev_X, dev_Y, cfg[\"batch_size\"])\n",
    "    print(epoch, f\"{score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can call `model.to_disk` or `model.to_bytes` to save the model weights to a directory or a bytestring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
