{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Like Polynomial Terms\n",
    "\n",
    "Remember in Algebra how you had to combine like terms to simplify problems? \n",
    "\n",
    "You'd see expressions like `60 + 2x^3 - 6x + x^3 + 17x` in which there are **5** total terms but only **4** are \"like terms\". \n",
    "\n",
    "`2x^3` and `x^3` are like, and `-6x` and `17x` are like, while `60` doesn't have any like siblings.\n",
    "\n",
    "Can we teach a model to predict that there are `4` like terms in the above expression?\n",
    "\n",
    "Let's give it a shot using [Mathy](https://mathy.ai) to generate math problems and [thinc](https://github.com/explosion/thinc) to build a regression model that outputs the number of like terms in each input problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install thinc mathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Text Inputs\n",
    "\n",
    "Mathy generates ascii-math problem texts that we need to encode into scalar values for the model to process. We'll one-hot encode the text by individual characters to keep things simple.\n",
    "\n",
    "For math problems our vocabulary will include all the characters of the alphabet, numbers 0-9, and the special characters used for operators like `*`, `/`, `.`, etc.\n",
    "\n",
    "Thinc has a `to_categorical` helper that we can use for converting values into one-hot arrays. Let's use that to write a function that accepts an input string and returns a one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.types import Array2d\n",
    "from thinc.api import Ops, get_current_ops, to_categorical\n",
    "\n",
    "vocab = \" .+-/^*()[]-01234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def encode_input(text: str) -> Array2d:\n",
    "    ops: Ops = get_current_ops()\n",
    "    vocab_indices: List[int] = [vocab.index(s) for s in text]\n",
    "    return to_categorical(ops.asarray(vocab_indices), n_classes=len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "Let's try it out on some fixed data to be sure it works. \n",
    "\n",
    "We should expect to see an output of one-hot arrays, one for each character in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "outputs = encode_input(\"4+2\")\n",
    "assert outputs[0].argmax() == vocab.index(\"4\")\n",
    "assert outputs[1].argmax() == vocab.index(\"+\")\n",
    "assert outputs[2].argmax() == vocab.index(\"2\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Math Problems\n",
    "\n",
    "We'll use Mathy to generate random polynomial problems with a variable number of like terms. The generated problems will act as training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Set\n",
    "import random\n",
    "from mathy.problems import gen_simplify_multiple_terms\n",
    "\n",
    "\n",
    "def generate_problems(number: int, exclude: Optional[Set[str]] = None) -> List[str]:\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    problems: List[str] = []\n",
    "    while len(problems) < number:\n",
    "        text, _ = gen_simplify_multiple_terms(\n",
    "            random.randint(3, 8), noise_probability=1.0, noise_terms=10\n",
    "        )\n",
    "        assert text not in exclude, \"duplicate problem generated!\"\n",
    "        problems.append(text)\n",
    "    return problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12f + -6624y + 11.8x + 5x + 6.7x + -3139x + 10q + 948s + -8685o^4 + 5.8u^2 + -2833f + 7j + 5.9f + 12n + -8291l + 11.8f + 10d^4 + -736t',\n",
       " '11p^4 + 1811d + 3z^4 + 9.0h + -648o + 0.8z^4 + 6p^4 + 4.7a + 7.0y^3 + -678m^4 + 11f + 3.2b + 3061p^4 + 2t + 9s + -28z^4',\n",
       " '10r + 2h + -6335b + 5597f + -5569p^3 + 8.2l^2 + 6.6c + 2844.4n + 4.2w + -4266q + 1431.4p^3 + 1.5k^3 + 11.4v',\n",
       " '5n + 3q + 6.7s^2 + 1k + 5.8g + 8192.0w + 7.5a + 1.1u + 6c + 3z + 6m^2 + 1487z + -3267m^2 + 4.2z + 2313.0m^2 + -5745z + 12f^4',\n",
       " '4147.9p + -7487m + -6963.9t^4 + 1.8q + -7945r^3 + -7471.8t^4 + 11.5l + 5.6l + 10l + 0.3z + 5.4y^2 + 7.8n + -8982t^4 + 2.4l + 2.7s + 1o + 6.6v',\n",
       " '6j^4 + 1.9s + -6310r^3 + 11.4a + -995d + 8969v^2 + 5.5s + 1m^2 + 1007u^4 + 9879g^4 + 4.9s + 8x + 3u^4 + 6754o + 2.5n^4 + 10.5u^4',\n",
       " '8.6f + 7.3d + 5n + 9d + 9.1z^3 + 9164y + 0.9d + 7663u^3 + -4640a^2 + -2512j + 3604n + 10b^3 + 5.9n + 10w + 5217d + -4344t + 10q + -1014n',\n",
       " '12a^2 + 6t^2 + 11w^2 + -7833x + 2b + 7.4h^3 + 4z^3 + 2q^4 + -7406o + 8.8y + -6884r + 6525y + 5r + 7219y + 4147s^4',\n",
       " '-848r^2 + 10h^3 + 5818o + 0.1f + 50t + 1p + 5207z + 7.4k^3 + 6.2w + 7g + 2.5q + 6o + 2c',\n",
       " '-8324w + 7447.1o + 8h + 6b^3 + 8219u + -498g + 11y^4 + 9b^3 + 7.6l + -6996r + 2d + 3w + 11q + 3.2m + -2805b^3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_problems(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Like Terms\n",
    "\n",
    "Now that we can generate input problems, we'll need a function that can count the like terms in each one and return the value for use as a label.\n",
    "\n",
    "To accomplish this we'll use a few helpers from mathy to enumerate the terms and compare them to see if they're like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from mathy import MathExpression, ExpressionParser, get_terms, get_term_ex, TermEx\n",
    "from mathy.problems import mathy_term_string\n",
    "\n",
    "parser = ExpressionParser()\n",
    "\n",
    "def count_like_terms(input_problem: str) -> int:\n",
    "    expression: MathExpression = parser.parse(input_problem)\n",
    "    term_nodes: List[MathExpression] = get_terms(expression)\n",
    "    node_groups: Dict[str, List[int]] = {}\n",
    "    for term_node in term_nodes:\n",
    "        ex: Optional[TermEx] = get_term_ex(term_node)\n",
    "        assert ex is not None, f\"invalid expression {ex}\"\n",
    "        key = mathy_term_string(variable=ex.variable, exponent=ex.exponent)\n",
    "        if key == \"\":\n",
    "            key = \"const\"\n",
    "        if key not in node_groups:\n",
    "            node_groups[key] = [term_node]\n",
    "        else:\n",
    "            node_groups[key].append(term_node)\n",
    "    like_terms = 0\n",
    "    for k, v in node_groups.items():\n",
    "        if len(v) <= 1:\n",
    "            continue\n",
    "        like_terms += len(v)\n",
    "    return like_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert count_like_terms(\"4x + 2y + q\") == 0\n",
    "assert count_like_terms(\"x + x + z\") == 2\n",
    "assert count_like_terms(\"4x + 2x + x + 7\") == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Problem/Answer pairs\n",
    "\n",
    "Now that we can generate problems, count the number of like terms in them, and encode their text into one-hot vectors, we have the pieces required to generate random problems and answers that we can train a neural network on.\n",
    "\n",
    "Let's write a function that will return a tuple of: the problem text, its encoded example form, and the output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from thinc.types import Array2d\n",
    "\n",
    "def to_example(input_problem: str) -> Tuple[str, Array2d, List[int]]:\n",
    "    one_hot = encode_input(input_problem)\n",
    "    like_terms = count_like_terms(input_problem)\n",
    "    return input_problem, one_hot, [like_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+2x [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]] [2]\n"
     ]
    }
   ],
   "source": [
    "text, X, Y = to_example(\"x+2x\")\n",
    "# text is preserved for debugging/visualization\n",
    "assert text == \"x+2x\"\n",
    "# X contains one-hot vectors\n",
    "assert X[0].argmax() == vocab.index(\"x\")\n",
    "# Y is the number of like terms\n",
    "assert Y[0] == 2\n",
    "print(text, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model\n",
    "\n",
    "Now that we can generate X/Y values, let's define our model and verify it can process a single input/output.\n",
    "\n",
    "For this we'll use Thinc and define a model that takes in an `Array2d` of examples and outputs an `Array1d` of scalar values, one for each example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.model import Model\n",
    "from thinc.types import Array2d, Array1d\n",
    "from thinc.api import chain, list2ragged, reduce_sum, ReLu\n",
    "\n",
    "\n",
    "def to_ints():\n",
    "    def forward(model: Model[Array2d, Array1d], Y: Array2d, is_train: bool):\n",
    "        Y = Y.astype(\"i\")\n",
    "        return Y, lambda dY: dY.astype(\"f\")\n",
    "\n",
    "    return Model(\"toint\", forward)\n",
    "\n",
    "\n",
    "def CountLikeTerms(n_hidden: int, dropout: float = 0.5) -> Model[List[Array2d], Array2d]:\n",
    "    return chain(\n",
    "        list2ragged(),\n",
    "        reduce_sum(),\n",
    "        ReLu(n_hidden, dropout=dropout),\n",
    "        ReLu(n_hidden),\n",
    "        ReLu(n_hidden),\n",
    "        ReLu(n_hidden),\n",
    "        ReLu(1),\n",
    "        to_ints(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "Let's pass an example through the model to make sure we have all the sizes right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, X, Y = to_example(\"14x + 2y - 3x + 7x\")\n",
    "m = CountLikeTerms(12)\n",
    "m.initialize([X], Y)\n",
    "assert m.predict([X]).shape == (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Datasets\n",
    "\n",
    "Now that we can generate examples and we have a model that can process them, let's generate random unique training and evaluation datasets.\n",
    "\n",
    "For this we'll write another helper function that can generate (n) training examples and respects an exclude list to avoid letting examples from the training/test sets overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Set, List\n",
    "from thinc.types import Array2d\n",
    "\n",
    "def generate_dataset(\n",
    "    size: int, exclude: Optional[Set[str]] = None\n",
    ") -> Tuple[List[str], List[Array2d], List[List[int]]]:\n",
    "    texts: List[str] = generate_problems(size)\n",
    "    examples: List[Array2d] = []\n",
    "    labels: List[List[int]] = []\n",
    "    for i, text in enumerate(texts):\n",
    "        text, x, y = to_example(text)\n",
    "        examples.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    return texts, examples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "Generate a small dataset to be sure everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['4453j + -7872q + -1522.0o + 0.9n + 4a + 6k^4 + 8l + 11t + 9l + 12t + 9097s^2 + 10u + 1g^2 + 5290f^4',\n",
       "  '2078z^3 + 7a + 0.1l + -1658k^3 + 8.4a + -1487.6z^3 + 8.9o^2 + -6395a + 9225a + 4938.2r^2 + 1g + 5663z^3 + 5x + 8733w + 10m + 9n^4 + 10y',\n",
       "  '3177p^3 + 7730.4f^3 + 9.3f^3 + -901y^4 + 8.5c^2 + 7b + 5f^3 + 1p^3 + 7p^3 + 1g + 3071f^3 + 7a + 3t + 1978n^3 + 5.4k^2 + 11o^2 + 7h^4'],\n",
       " [array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)],\n",
       " [[4], [7], [7]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dataset(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance\n",
    "\n",
    "We're almost ready to train our model, we just need to write a function that will check a given trained model against a given dataset and return a 0-1 score of how accurate it was.\n",
    "\n",
    "We'll use this function to print the score as training progresses and print final test predictions at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.model import Model\n",
    "from thinc.types import Array2d, Array1d\n",
    "from wasabi import msg\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: Model[Array2d, Array1d],\n",
    "    *,\n",
    "    print_problems: bool = False,\n",
    "    texts: List[str],\n",
    "    X: Array2d,\n",
    "    Y: List[int],\n",
    "):\n",
    "    Yeval = model.predict(X)\n",
    "    correct_count = 0\n",
    "    print_n = 10\n",
    "    if print_problems:\n",
    "        msg.divider(f\"eval samples max({print_n})\")\n",
    "    for text, y_answer, y_guess in zip(texts, Y, Yeval):\n",
    "        y_guess = round(float(y_guess))\n",
    "        correct = y_guess == int(y_answer[0])\n",
    "        print_fn = msg.fail\n",
    "        if correct:\n",
    "            correct_count += 1\n",
    "            print_fn = msg.good\n",
    "        if print_problems and print_n > 0:\n",
    "            print_n -= 1\n",
    "            print_fn(f\"Answer[{y_answer[0]}] Guess[{y_guess}] Text: {text}\")\n",
    "    return correct_count / len(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "Let's try it out with an untrained model and expect to see a really sad score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, X, Y = generate_dataset(128)\n",
    "m = CountLikeTerms(12)\n",
    "m.initialize(X, Y)\n",
    "# Assume the model should do so poorly as to round down to 0\n",
    "assert round(evaluate_model(m, texts=texts, X=X, Y=Y)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Knerating train dataset with 50000 examples...\u001b[38;5;2m✔ Train set created with 50000 examples.\u001b[0m\n",
      "\u001b[2Knerating eval dataset with 128 examples...\u001b[38;5;2m✔ Eval set created with 128 examples.\u001b[0m\n",
      "0\t0.58\t89694.00\n",
      "1\t0.59\t29407.00\n",
      "2\t0.66\t21025.00\n",
      "3\t0.66\t16449.00\n",
      "4\t0.77\t13958.00\n",
      "5\t0.79\t11970.00\n",
      "6\t0.85\t11160.00\n",
      "7\t0.81\t10352.00\n",
      "8\t0.80\t9941.00\n",
      "9\t0.84\t9795.00\n",
      "10\t0.80\t9465.00\n",
      "11\t0.86\t9249.00\n",
      "\u001b[1m\n",
      "============================ eval samples max(10) ============================\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[4]] Guess[4] Text: 6.9k + -4124l^4 + 4d + 10m^3 + 0.2x + 6q +\n",
      "-7098p^2 + 7.3l^4 + 4o + 4r + 5975v^4 + 11s^2 + 0.4g^3 + 10.4g^3\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[4]] Guess[4] Text: 2510g + -8759m + 12t + 9y + 0.5c + 1880a +\n",
      "10.9j^4 + 1q + 1143.9c + 4.0n^4 + 3.7y + 8356s + 5u^3 + 5.0x\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[2]] Guess[2] Text: 9h + 1.8n^3 + 11.0z + 7969w^4 + -4606j +\n",
      "3604v^3 + 5573.4g + -8408u + 4s^4 + 2u + 6f + 10x^2 + 6p^2\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[7]] Guess[7] Text: 8z + 9m + 6.4y + -7238.6d^2 + 9.0z + 11f +\n",
      "6u^4 + 2w^4 + 12l + 1022d^2 + 4t + 8335x + 7.6z + 8430n + -6979z + 9004c^2 +\n",
      "-6079d^2\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[4]] Guess[4] Text: 3374x^3 + 1.4d + 4r + 8.3y + 6l^2 + 0.7c^3\n",
      "+ 1.6x^3 + 5t + 2.4d + 11.3n^4 + -9231p + 7g + 1134o + -8447f\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[6]] Guess[6] Text: 7l^2 + 583.6n + 685.8a + -5088.8p + 9.8b^3\n",
      "+ 5.5j + 524n + 1.4y^2 + 9x + -3710b^3 + 7484h^2 + 2b^3 + 1.6n + 1839m + 4d^2 +\n",
      "-3209u\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[[4]] Guess[3] Text: 4p + 9181b + 11.1s + -7624g + 3.9u + 10.3q\n",
      "+ -6195.1m + -9391l^3 + 11p + 4.4h + 0.2a + 10.1k^4 + 5.4j + 6.5q\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[2]] Guess[2] Text: 6863h + -7685x + 8559u + -457j + 6.2k +\n",
      "11z + 1o^3 + -9991m + 12f + 6m + 2.1b + 4.6t + 8.4y\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[[2]] Guess[3] Text: -1092o + 7080n^2 + 8.8k^3 + 6.0a + 3.7b^4\n",
      "+ 7d + 5.2g + 6694f^3 + -7673z + 4h + 9y + -4915.6r + 5y\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[[6]] Guess[6] Text: 3.8t^3 + 5161d^2 + 1f + -6482o + -8956n^4\n",
      "+ 10.4r + 7999.7m + 4542h^4 + 1.2u + 4448s + -576u + 12s + -7281u + 5906s +\n",
      "2468y + 7993v\u001b[0m\n",
      "Score: 0.8515625\n"
     ]
    }
   ],
   "source": [
    "from typing import Set\n",
    "from thinc.api import Adam, fix_random_seed\n",
    "from wasabi import msg\n",
    "import numpy\n",
    "\n",
    "# Set the random seed to make results more reproducible\n",
    "fix_random_seed(1234)\n",
    "batch_size = 12\n",
    "num_iters = 12\n",
    "train_size = 50000\n",
    "test_size = 128\n",
    "seen_texts: Set[str] = set()\n",
    "with msg.loading(f\"Generating train dataset with {train_size} examples...\"):\n",
    "    (train_texts, train_X, train_y) = generate_dataset(train_size, seen_texts)\n",
    "msg.good(f\"Train set created with {train_size} examples.\")\n",
    "with msg.loading(f\"Generating eval dataset with {test_size} examples...\"):\n",
    "    (eval_texts, eval_X, eval_y) = generate_dataset(test_size, seen_texts)\n",
    "msg.good(f\"Eval set created with {test_size} examples.\")\n",
    "model = CountLikeTerms(512)\n",
    "model.initialize(train_X[:2], model.ops.asarray(train_y[:2]))\n",
    "optimizer = Adam(2e-3)\n",
    "for n in range(num_iters):\n",
    "    loss = 0.0\n",
    "    batches = model.ops.multibatch(batch_size, train_X, train_y, shuffle=True)\n",
    "    for X, Y in batches:\n",
    "        Y = model.ops.asarray(Y, dtype=\"float32\")\n",
    "        Yh, backprop = model.begin_update(X)\n",
    "        err = Yh - Y\n",
    "        backprop(err)\n",
    "        loss += (err ** 2).sum()\n",
    "        model.finish_update(optimizer)\n",
    "    score = evaluate_model(model, texts=eval_texts, X=eval_X, Y=eval_y)\n",
    "    print(f\"{n}\\t{score:.2f}\\t{loss:.2f}\")\n",
    "\n",
    "Yeval = model.predict(eval_X)\n",
    "score = evaluate_model(\n",
    "    model, print_problems=True, texts=eval_texts, X=eval_X, Y=eval_y\n",
    ")\n",
    "print(f\"Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Exercise\n",
    "\n",
    "Rewrite the model to encode the whole expression with a BiLSTM, and then generate pairs of terms, using the BiLSTM vectors. Over each pair of terms, predict whether the terms are alike or unlike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from thinc.types import Array2d, Ragged\n",
    "from thinc.model import Model\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Comparisons:\n",
    "    data: Array2d  # Batch of vectors for each pair\n",
    "    indices: Array2d  # Int array of shape (N, 3), showing the (batch, term1, term2) positions\n",
    "\n",
    "def pairify() -> Model[Ragged, Comparisons]:\n",
    "    \"\"\"Create pair-wise comparisons for items in a sequence. For each sequence of N\n",
    "    items, there will be (N**2-N)/2 comparisons.\"\"\"\n",
    "    ...\n",
    "\n",
    "def predict_over_pairs(model: Model[Array2d, Array2d]) -> Model[Comparisons, Comparisons]:\n",
    "    \"\"\"Apply a prediction model over a batch of comparisons. Outputs a Comparisons\n",
    "    object where the data is the scores. The prediction model should predict over\n",
    "    two classes, True and False.\"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinc",
   "language": "python",
   "name": "thinc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
