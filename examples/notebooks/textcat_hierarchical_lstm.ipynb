{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with hierarchical LSTM\n",
    "\n",
    "TODO: intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use Thinc's `prefer_gpu` helper to make sure we're performing operations **on GPU if available**. The function should be called right after importing Thinc, and it returns a boolean indicating whether the GPU has been activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import prefer_gpu\n",
    "is_gpu = prefer_gpu()\n",
    "print(\"GPU:\", is_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the model\n",
    "\n",
    "Each batch of inputs will be received as a list of spaCy [`Doc`](https://spacy.io/api/doc) objects, and the model will need to output a vector of scores for each entry in the list. Let's start planning this out, using type-annotations to keep track of the inputs and outputs:\n",
    "\n",
    "```\n",
    "TextClassifier(...) -> Model[List[Doc], Array2d]\n",
    "```\n",
    "\n",
    "Our model will need some sort of embedding function, and some function that re-encodes the vectors based on context (such as a BiLSTM, CNN, etc). For the encoding layer, it’s usually best to work on padded batches, so that component should take a `Padded` object as input, and return a `Padded` object as output. So let’s write our function expecting the embed layer to return that type, and we’ll expect our reduction layer to transform from `Padded` to `Array2d`, which is what the prediction layer will expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.api import Model, Padded, chain\n",
    "from thinc.types import Array2d\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def TextClassifier(\n",
    "    embed: Model[List[Doc], Padded],\n",
    "    encode: Model[Padded, Padded],\n",
    "    reduce: Model[Padded, Array2d],\n",
    "    predict: Model[Array2d, Array2d]\n",
    ") -> Model[List[Doc], Array2d]:\n",
    "    return chain(embed, encode, reduce, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `WordEmbed` layer\n",
    "\n",
    "Now that we know all our input and output types, we can design functions to build the various components. There could be any number of combinations, but here are some reasonable defaults. The `with_array` wrapper transforms to an `Array2d`, and then reverses the transformation on the way back out. For the `Padded` object, this is just a reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import Model, chain, docs2arrays, list2padded, with_array, chain, Embed, Padded\n",
    "from spacy.tokens import Doc\n",
    "from spacy.attrs import LOWER\n",
    "\n",
    "# TODO: remap_ids, docs2arrays\n",
    "\n",
    "def WordEmbed(width, vocab) -> Model[List[Doc], Padded]:\n",
    "    return chain(\n",
    "        docs2arrays([LOWER]),\n",
    "        list2padded(),\n",
    "        with_array( # Padded -> Padded\n",
    "            chain(remap_ids(vocab), Embed(nO=width, nV=len(vocab)))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `docs2arrays` layer uses spaCy's [`Doc.to_array`](https://spacy.io/api/doc#to_array) method, giving you an array\n",
    "with the features you asked for. In this case, we'll use the lowercased form of the word. We then sort the batch by length and pad it, returning the results in Thinc's `Padded` dataclass. At this point, the values in our batch are 64-bit hashes, as that's what spaCy's `Token.lower` attribute returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the `TextClassifier`\n",
    "\n",
    "With our `WordEmbed` layer complete, we can put in some other reasonable choices for our text classifier, and create a model instance.\n",
    "\n",
    "\n",
    "The `PyTorchLSTM` layer wraps PyTorch's LSTM implementation, taking care of the input and output transformations to work on Thinc's `Padded` type. We then use the `concatenate` combinator to build a feature representation using both max pooling and mean pooling. Pooling operations lose a lot of information, so using two together can result in slightly higher accuracy.\n",
    "\n",
    "The `concatenate` combinator is an example of the type of relationship that you'd express in terms of instances: in PyTorch you'd write something like `Y = torch.concat(X.mean(axis=0), X.max(axis=0))`. This approach works fine, but the data variables (`X` and `Y`) can make it more difficult to see the relationships – and it's the relationships, the network topology, that you are usually most interested when you're reviewing the network. Once you get used to it, writing the network this way is also very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from thinc.api import PyTorchLSTM, concatenate, MaxPool, MeanPool, chain, residual, ReLu, Softmax\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "vocab = {nlp.vocab.strings[\"hello\"]: 0, nlp.vocab.strings[\"world\"]: 1}\n",
    "width = 128\n",
    "n_class = 4\n",
    "\n",
    "embed = WordEmbed(width, vocab)\n",
    "encode = PyTorchLSTM(nO=width, nI=width, dropout=0.2, bi=True)\n",
    "reduce = concatenate(MaxPool(), MeanPool())\n",
    "predict = chain(\n",
    "    residual(ReLu(nO=width*2, dropout=0.2, normalize=True)),\n",
    "    Softmax(n_class)\n",
    ")\n",
    "\n",
    "model = TextClassifier(embed, encode, reduce, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, we then call its `initialize` method with a small batch of data, which gives layers a chance to guess missing dimensions and allocate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [nlp.make_doc(\"hello world\"), nlp.make_doc(\"example 2\")]\n",
    "labels = model.ops.asarray([[1.0, 0.0], [0.0, 1.0]], dtype=\"f\")\n",
    "model.initialize(X=inputs, Y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating character features\n",
    "\n",
    "Let's make the example more complex, to see how different requirements are handled. What if we wanted to incorporate character features into the embedding layer? We still want to embed the word IDs, but we _also_ want to run some sort of character-sensitive model, and combine the two representations before passing the result forward into the rest of the network.\n",
    "\n",
    "There are lots of ways to do character-based features, but for this example, we'll encode the strings to UTF-8 byte sequences, embed the bytes, encode the sequences using an LSTM, and use the final hidden state as the character-based vector. We'll then sum the character-based vectors with the embedded word IDs.\n",
    "\n",
    "For this new model, we'll have to do some refactoring of our `TextClassifier` and `WordEmbed` components. We want the two embedding layers to take the same input and output types, so that we can combine them easily. The most convenient signature would be `Model[List[str], Array2d]`: a flat list of strings for the whole batch as input, and a single array for the batch as output.\n",
    "\n",
    "This requires a few changes to the `WordEmbed` layer we defined earlier. We'll remove the `list2padded` step, and instead of `docs2strings`, we'll define our own little lower-casing operation and apply the `strings2arrays` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.api import Model, chain, strings2arrays, with_array, chain, Embed\n",
    "from thinc.types import Array2d\n",
    "\n",
    "def WordEmbed(width, vocab) -> Model[List[str], Array2d]:\n",
    "    return chain(\n",
    "        lower_case(),\n",
    "        strings2arrays(),\n",
    "        with_array(\n",
    "            chain(remap_ids(vocab), Embed(nO=width, nV=len(vocab)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "def lower_case() -> Model[List[str], List[str]]:\n",
    "    return Model(\"lower_case\", _lower_case_forward)\n",
    "\n",
    "def _lower_case_forward(model, strings, is_train):\n",
    "    def backprop(d_strings):\n",
    "        return []\n",
    "    \n",
    "    return [string.lower() for string in strings], backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lower_case` transformation is not differentiable, but we still need to pass a callback for the backward pass, which should return the same type as the input. The `CharacterEmbed` layer also needs a little non-differentiable transformation, to encode the strings into UTF-8 and convert the bytes into arrays. Once we have that, we can chain the pieces together, much as we did for the document model, but with a different reduction strategy: instead of using reducing to the mean and max, we'll reduce by just taking the last vector of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharacterEmbed(width, depth) -> Model[List[str], Array2d]:\n",
    "    return chain(\n",
    "        strings2utf8(),\n",
    "        list2padded(),\n",
    "        with_array(Embed(width, 256)),\n",
    "        PyTorchLSTM(width, width, depth=depth),\n",
    "        reduce_nth(-1)\n",
    "    )\n",
    "\n",
    "def strings2utf8() -> Model[List[str]], List[Array2d]]:\n",
    "    return Model(\"strings2utf8\", forward)\n",
    "\n",
    "\n",
    "def forward(model: Model[List[str], List[Array2d], words: List[str], is_train: bool):\n",
    "    def backprop(d_output: List[Array2d]) -> List[str]:\n",
    "        return []\n",
    "                         \n",
    "    utf8_arrays: List[str]] = []\n",
    "    for word in words:\n",
    "        chars = word.encode(\"utf8\")\n",
    "        ints = list(map(int.from_bytes, chars))\n",
    "        arr = model.ops.asarray(ints, dtype=\"i\").reshape((-1, 1))\n",
    "        utf8_arrays.append(arr)\n",
    "    return utf8_arrays, backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wire everything together, we need to flatten the nested lists on the way into the embedding layer, and split the output array into lists of the correct length. The `with_flatten` function provides the necessary transformation. With all the pieces in place, we can finish updating our `TextClassifier` function, and pass in our combined embedding model, with one last finishing touch: the addition of a caching operation, `uniqued`, around the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.api import Model, Padded, chain, docs2strings, with_flatten, list2padded, uniqued, PyTorchBiLSTM, concatenate, MaxPool, MeanPool, chain, residual, ReLu, Softmax\n",
    "from thinc.types import Array2d\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def TextClassifier(\n",
    "    embed: Model[List[str], Array2d],\n",
    "    encode: Model[Padded, Padded],\n",
    "    reduce: Model[Padded, Array2d],\n",
    "    predict: Model[Array2d, Array2d]\n",
    ") -> Model[List[Doc], Array2d]:\n",
    "    return chain(\n",
    "        docs2strings(),      # List[Doc] -> List[List[str]]\n",
    "        with_flatten(embed), # List[List[str]] -> List[Array2d]\n",
    "        list2padded(),\n",
    "        encode,\n",
    "        reduce,\n",
    "        predict\n",
    "    )\n",
    "\n",
    "# TODO: What's word_embed? What's char_embed?\n",
    "\n",
    "model = TextClassifer(\n",
    "    uniqued(add(word_embed, char_embed)),\n",
    "    PyTorchBiLSTM(width, width, dropout=0.0),\n",
    "    concatenate(MaxPool(), MeanPool()),\n",
    "    chain(\n",
    "        residual(ReLu(nO=width*2, dropout=0.2, normalize=True)),\n",
    "        Softmax(n_class)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training the model\n",
    "\n",
    "TODO: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('.env': venv)",
   "language": "python",
   "name": "python37264bitenvvenv9eb7caf448714d3f8d7dc6238703fa1e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
